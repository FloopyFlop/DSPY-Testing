{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d025f9",
   "metadata": {},
   "source": [
    "\n",
    "# DSPy \"Under the Hood\" Demo with Traces and Scores\n",
    "\n",
    "This notebook gives a **small, concrete, step by step demo** of how DSPy:\n",
    "\n",
    "* runs a simple program on examples\n",
    "* records internal traces of each module call\n",
    "* uses a metric to accept or reject traces\n",
    "* can be optimized with an optimizer like `BootstrapFewShotWithRandomSearch`\n",
    "\n",
    "The demo uses a very small GSM8K style math task so you can actually print and inspect everything.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0977a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you do not have DSPy installed, uncomment this cell and run it.\n",
    "# It is left commented so that you can control your own environment.\n",
    "# !pip install -qU dspy-ai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be3e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dspy\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d618b4",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Configure DSPy with your local Ollama model\n",
    "\n",
    "This uses the configuration you specified, pointing DSPy to `qwen3:30b` running on `localhost:11434`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8f9faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure DSPy to use Ollama with qwen3:30b\n",
    "ollama_model = dspy.LM(\n",
    "    model='ollama/qwen3:30b',\n",
    "    api_base='http://localhost:11434',\n",
    "    api_key=''  # Ollama does not require an API key\n",
    ")\n",
    "\n",
    "dspy.configure(lm=ollama_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3fe541",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Define a tiny GSM8K style dataset and metric\n",
    "\n",
    "We will work with a very small synthetic math dataset so that traces stay readable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "868fe22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# A tiny synthetic \"GSM8K style\" dataset\n",
    "train_examples = [\n",
    "    dspy.Example(\n",
    "        question=\"Tom has 3 apples and buys 2 more. How many apples does he have now?\",\n",
    "        answer=\"5\",\n",
    "    ).with_inputs(\"question\"),\n",
    "    dspy.Example(\n",
    "        question=\"A box has 10 candies and you eat 4. How many are left?\",\n",
    "        answer=\"6\",\n",
    "    ).with_inputs(\"question\"),\n",
    "    dspy.Example(\n",
    "        question=\"Sara has 12 pencils, gives 3 to her friend, and buys 5 more. How many pencils does she have now?\",\n",
    "        answer=\"14\",\n",
    "    ).with_inputs(\"question\"),\n",
    "]\n",
    "\n",
    "dev_examples = [\n",
    "    dspy.Example(\n",
    "        question=\"Mike has 7 oranges and buys 3 more. How many does he have?\",\n",
    "        answer=\"10\",\n",
    "    ).with_inputs(\"question\"),\n",
    "    dspy.Example(\n",
    "        question=\"There are 15 cookies. You eat 7. How many remain?\",\n",
    "        answer=\"8\",\n",
    "    ).with_inputs(\"question\"),\n",
    "]\n",
    "\n",
    "len(train_examples), len(dev_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6868ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A simple numeric accuracy metric\n",
    "def gsm8k_metric(example, prediction, trace=None):\n",
    "    \"\"\"Return 1 if the numeric answer matches, else 0.\n",
    "\n",
    "    This is intentionally simple so we can see pass or fail clearly.\n",
    "    \"\"\"\n",
    "    pred_text = str(getattr(prediction, \"answer\", \"\")).strip()\n",
    "    gold_text = str(example.answer).strip()\n",
    "\n",
    "    # grab the first integer that appears\n",
    "    import re\n",
    "\n",
    "    def extract_int(text):\n",
    "        m = re.search(r\"-?\\d+\", text)\n",
    "        return int(m.group()) if m else None\n",
    "\n",
    "    pred_val = extract_int(pred_text)\n",
    "    gold_val = extract_int(gold_text)\n",
    "\n",
    "    return int(pred_val is not None and gold_val is not None and pred_val == gold_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef01e5d",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Define a simple DSPy program\n",
    "\n",
    "This program is a single `ChainOfThought` module that maps:\n",
    "\n",
    "```text\n",
    "question -> answer\n",
    "```\n",
    "\n",
    "We wrap it in a module so we can more easily inspect its behavior and later compile it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3faaead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cot.predict = Predict(StringSignature(question -> reasoning, answer\n",
       "    instructions='Given the fields `question`, produce the fields `answer`.'\n",
       "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
       "    reasoning = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${reasoning}', '__dspy_field_type': 'output'})\n",
       "    answer = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Answer:', 'desc': '${answer}'})\n",
       "))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class MathCoT(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Chain of Thought from question to answer\n",
    "        self.cot = dspy.ChainOfThought(\"question -> answer\")\n",
    "\n",
    "    def forward(self, question: str):\n",
    "        # Just delegate to the ChainOfThought module\n",
    "        pred = self.cot(question=question)\n",
    "        return pred\n",
    "\n",
    "math_program = MathCoT()\n",
    "math_program\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a414df3",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Baseline behavior (no optimization)\n",
    "\n",
    "First, run the raw program on the tiny train and dev sets to see how it behaves before any optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bc36da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline on train:\n",
      "Example 0:\n",
      "  Q: Tom has 3 apples and buys 2 more. How many apples does he have now?\n",
      "  Predicted answer: 5\n",
      "  Gold answer: 5\n",
      "  Metric score: 1\n",
      "------------------------------------------------------------\n",
      "Example 1:\n",
      "  Q: A box has 10 candies and you eat 4. How many are left?\n",
      "  Predicted answer: 6\n",
      "  Gold answer: 6\n",
      "  Metric score: 1\n",
      "------------------------------------------------------------\n",
      "Example 2:\n",
      "  Q: Sara has 12 pencils, gives 3 to her friend, and buys 5 more. How many pencils does she have now?\n",
      "  Predicted answer: 14\n",
      "  Gold answer: 14\n",
      "  Metric score: 1\n",
      "------------------------------------------------------------\n",
      "Average score on 3 examples: 1.000\n",
      "\n",
      "Baseline on dev:\n",
      "Example 0:\n",
      "  Q: Mike has 7 oranges and buys 3 more. How many does he have?\n",
      "  Predicted answer: 10\n",
      "  Gold answer: 10\n",
      "  Metric score: 1\n",
      "------------------------------------------------------------\n",
      "Example 1:\n",
      "  Q: There are 15 cookies. You eat 7. How many remain?\n",
      "  Predicted answer: 8\n",
      "  Gold answer: 8\n",
      "  Metric score: 1\n",
      "------------------------------------------------------------\n",
      "Average score on 2 examples: 1.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_program(program, dataset, metric_fn):\n",
    "    results = []\n",
    "    for i, ex in enumerate(dataset):\n",
    "        with dspy.context(trace=[]):\n",
    "            pred = program(question=ex.question)\n",
    "            trace = dspy.settings.trace.copy()\n",
    "\n",
    "        score = metric_fn(ex, pred, trace)\n",
    "        results.append(score)\n",
    "\n",
    "        print(f\"Example {i}:\")\n",
    "        print(\"  Q:\", ex.question)\n",
    "        print(\"  Predicted answer:\", getattr(pred, \"answer\", None))\n",
    "        print(\"  Gold answer:\", ex.answer)\n",
    "        print(\"  Metric score:\", score)\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    avg_score = sum(results) / max(len(results), 1)\n",
    "    print(f\"Average score on {len(dataset)} examples: {avg_score:.3f}\")\n",
    "    return avg_score\n",
    "\n",
    "print(\"Baseline on train:\")\n",
    "_ = evaluate_program(math_program, train_examples, gsm8k_metric)\n",
    "\n",
    "print(\"\\nBaseline on dev:\")\n",
    "_ = evaluate_program(math_program, dev_examples, gsm8k_metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd6fffb",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Manual tracing and simple \"backtracking\" demo\n",
    "\n",
    "In this section we will:\n",
    "\n",
    "* run the program on a single example multiple times\n",
    "* record the trace for each attempt\n",
    "* evaluate the metric\n",
    "* stop when we get a passing trace\n",
    "\n",
    "This imitates what DSPy does in `BootstrapFewShot`: it keeps only traces whose final prediction passes the metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8f66690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: A box has 10 candies and you eat 4. How many are left?\n",
      "Gold answer: 6\n",
      "\n",
      "Attempt 1:\n",
      "  Predicted answer: 6\n",
      "  Metric score: 1\n",
      "  Trace:\n",
      "    Step 0: module=Predict\n",
      "      inputs:  {'question': 'A box has 10 candies and you eat 4. How many are left?'}\n",
      "      outputs: {'reasoning': 'The box initially contains 10 candies. After eating 4 candies, the remaining number is calculated by subtracting 4 from 10. Thus, 10 - 4 = 6.', 'answer': '6'}\n",
      "\n",
      "  -> Accepted trace for this example.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pprint import pprint\n",
    "\n",
    "def run_with_trace(program, example, max_attempts=3):\n",
    "    accepted_traces = []\n",
    "    print(\"Question:\", example.question)\n",
    "    print(\"Gold answer:\", example.answer)\n",
    "    print()\n",
    "\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        with dspy.context(trace=[]):\n",
    "            pred = program(question=example.question)\n",
    "            trace = dspy.settings.trace.copy()\n",
    "\n",
    "        score = gsm8k_metric(example, pred, trace)\n",
    "\n",
    "        print(f\"Attempt {attempt}:\")\n",
    "        print(\"  Predicted answer:\", getattr(pred, \"answer\", None))\n",
    "        print(\"  Metric score:\", score)\n",
    "        print(\"  Trace:\")\n",
    "        for j, (mod, inputs, outputs) in enumerate(trace):\n",
    "            print(f\"    Step {j}: module={type(mod).__name__}\")\n",
    "            print(f\"      inputs:  {inputs}\")\n",
    "            print(f\"      outputs: {dict(outputs)}\")\n",
    "        print()\n",
    "\n",
    "        if score == 1:\n",
    "            accepted_traces.append(trace)\n",
    "            print(\"  -> Accepted trace for this example.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"  -> Rejected trace, trying again (if attempts remain).\")\n",
    "            print()\n",
    "\n",
    "    return accepted_traces\n",
    "\n",
    "# Try this on the second training example (the candy problem)\n",
    "accepted = run_with_trace(math_program, train_examples[1], max_attempts=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da064f78",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Collecting \"good\" traces like a tiny BootstrapFewShot\n",
    "\n",
    "Now let us collect at most one accepted trace per training example, using the simple loop from above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f6ce798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Question: Tom has 3 apples and buys 2 more. How many apples does he have now?\n",
      "Gold answer: 5\n",
      "\n",
      "Attempt 1:\n",
      "  Predicted answer: 5\n",
      "  Metric score: 1\n",
      "  Trace:\n",
      "    Step 0: module=Predict\n",
      "      inputs:  {'question': 'Tom has 3 apples and buys 2 more. How many apples does he have now?'}\n",
      "      outputs: {'reasoning': 'Tom starts with 3 apples and buys 2 more. To find the total, add the two quantities: 3 + 2 = 5.', 'answer': '5'}\n",
      "\n",
      "  -> Accepted trace for this example.\n",
      "================================================================================\n",
      "Question: A box has 10 candies and you eat 4. How many are left?\n",
      "Gold answer: 6\n",
      "\n",
      "Attempt 1:\n",
      "  Predicted answer: 6\n",
      "  Metric score: 1\n",
      "  Trace:\n",
      "    Step 0: module=Predict\n",
      "      inputs:  {'question': 'A box has 10 candies and you eat 4. How many are left?'}\n",
      "      outputs: {'reasoning': 'The box initially contains 10 candies. After eating 4 candies, the remaining number is calculated by subtracting 4 from 10. Thus, 10 - 4 = 6.', 'answer': '6'}\n",
      "\n",
      "  -> Accepted trace for this example.\n",
      "================================================================================\n",
      "Question: Sara has 12 pencils, gives 3 to her friend, and buys 5 more. How many pencils does she have now?\n",
      "Gold answer: 14\n",
      "\n",
      "Attempt 1:\n",
      "  Predicted answer: 14\n",
      "  Metric score: 1\n",
      "  Trace:\n",
      "    Step 0: module=Predict\n",
      "      inputs:  {'question': 'Sara has 12 pencils, gives 3 to her friend, and buys 5 more. How many pencils does she have now?'}\n",
      "      outputs: {'reasoning': 'Sara starts with 12 pencils. She gives away 3, so 12 - 3 = 9. Then she buys 5 more, so 9 + 5 = 14. Therefore, she has 14 pencils now.', 'answer': '14'}\n",
      "\n",
      "  -> Accepted trace for this example.\n",
      "\n",
      "Number of accepted traces: 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def collect_bootstrapped_traces(program, dataset, max_attempts=3):\n",
    "    all_traces = []\n",
    "    for ex in dataset:\n",
    "        print(\"=\" * 80)\n",
    "        traces = run_with_trace(program, ex, max_attempts=max_attempts)\n",
    "        if traces:\n",
    "            all_traces.append(traces[0])\n",
    "    return all_traces\n",
    "\n",
    "bootstrapped_traces = collect_bootstrapped_traces(math_program, train_examples, max_attempts=3)\n",
    "print(\"\\nNumber of accepted traces:\", len(bootstrapped_traces))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c07a5",
   "metadata": {},
   "source": [
    "\n",
    "At this point, `bootstrapped_traces` is a list of traces, and each trace is a list of\n",
    "`(module, inputs, outputs)` triples that passed the metric.\n",
    "\n",
    "Next, we will switch back to **real DSPy optimizers** and see how they automate this process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91333f3",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Real DSPy optimization with `BootstrapFewShotWithRandomSearch`\n",
    "\n",
    "We will now:\n",
    "\n",
    "* define an optimizer\n",
    "* compile the program on the tiny train set\n",
    "* evaluate before and after on the dev set\n",
    "* inspect which internal modules got demonstrations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d80c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 2 traces per predictor.\n",
      "Will attempt to bootstrap 4 candidate sets.\n",
      "Average Metric: 2.00 / 2 (100.0%): 100%|██████████| 2/2 [00:00<00:00, 1611.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/13 01:17:33 INFO dspy.evaluate.evaluate: Average Metric: 2 / 2 (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best score: 100.0 for seed -3\n",
      "Scores so far: [100.0]\n",
      "Best score so far: 100.0\n",
      "Average Metric: 2.00 / 2 (100.0%): 100%|██████████| 2/2 [00:13<00:00,  6.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/13 01:17:47 INFO dspy.evaluate.evaluate: Average Metric: 2 / 2 (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [100.0, 100.0]\n",
      "Best score so far: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:07<00:03,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
      "Average Metric: 2.00 / 2 (100.0%): 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/13 01:18:02 INFO dspy.evaluate.evaluate: Average Metric: 2 / 2 (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [100.0, 100.0, 100.0]\n",
      "Best score so far: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:06<00:03,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
      "Average Metric: 2.00 / 2 (100.0%): 100%|██████████| 2/2 [00:08<00:00,  4.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/13 01:18:17 INFO dspy.evaluate.evaluate: Average Metric: 2 / 2 (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [100.0, 100.0, 100.0, 100.0]\n",
      "Best score so far: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:00<00:00, 65.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Average Metric: 2.00 / 2 (100.0%): 100%|██████████| 2/2 [00:07<00:00,  3.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/13 01:18:24 INFO dspy.evaluate.evaluate: Average Metric: 2 / 2 (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "Best score so far: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:00<00:00, 60.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Average Metric: 2.00 / 2 (100.0%): 100%|██████████| 2/2 [00:00<00:00, 281.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/13 01:18:24 INFO dspy.evaluate.evaluate: Average Metric: 2 / 2 (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "Best score so far: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:00<00:00, 59.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Average Metric: 2.00 / 2 (100.0%): 100%|██████████| 2/2 [00:00<00:00, 471.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/13 01:18:24 INFO dspy.evaluate.evaluate: Average Metric: 2 / 2 (100.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores so far: [100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "Best score so far: 100.0\n",
      "7 candidate programs found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cot.predict = Predict(StringSignature(question -> reasoning, answer\n",
       "    instructions='Given the fields `question`, produce the fields `answer`.'\n",
       "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
       "    reasoning = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${reasoning}', '__dspy_field_type': 'output'})\n",
       "    answer = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Answer:', 'desc': '${answer}'})\n",
       "))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "optimizer = BootstrapFewShotWithRandomSearch(\n",
    "    metric=gsm8k_metric,\n",
    "    max_bootstrapped_demos=2,\n",
    "    num_candidate_programs=4,\n",
    "    num_threads=1,\n",
    ")\n",
    "\n",
    "# For this tiny demo, use the same data split for train and val, or slice as you like\n",
    "compiled_math_program = optimizer.compile(\n",
    "    student=MathCoT(),\n",
    "    trainset=train_examples,\n",
    "    valset=dev_examples,\n",
    ")\n",
    "\n",
    "compiled_math_program\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8f4740a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev performance BEFORE optimization:\n",
      "Example 0:\n",
      "  Q: Mike has 7 oranges and buys 3 more. How many does he have?\n",
      "  Predicted answer: 10\n",
      "  Gold answer: 10\n",
      "  Metric score: 1\n",
      "------------------------------------------------------------\n",
      "Example 1:\n",
      "  Q: There are 15 cookies. You eat 7. How many remain?\n",
      "  Predicted answer: 8\n",
      "  Gold answer: 8\n",
      "  Metric score: 1\n",
      "------------------------------------------------------------\n",
      "Average score on 2 examples: 1.000\n",
      "\n",
      "Dev performance AFTER optimization:\n",
      "Example 0:\n",
      "  Q: Mike has 7 oranges and buys 3 more. How many does he have?\n",
      "  Predicted answer: 10\n",
      "  Gold answer: 10\n",
      "  Metric score: 1\n",
      "------------------------------------------------------------\n",
      "Example 1:\n",
      "  Q: There are 15 cookies. You eat 7. How many remain?\n",
      "  Predicted answer: 8\n",
      "  Gold answer: 8\n",
      "  Metric score: 1\n",
      "------------------------------------------------------------\n",
      "Average score on 2 examples: 1.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Dev performance BEFORE optimization:\")\n",
    "_ = evaluate_program(math_program, dev_examples, gsm8k_metric)\n",
    "\n",
    "print(\"\\nDev performance AFTER optimization:\")\n",
    "_ = evaluate_program(compiled_math_program, dev_examples, gsm8k_metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3283eebc",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Inspect which modules got demonstrations\n",
    "\n",
    "Every `Predict` like module inside your program can have its own pool of demonstrations.\n",
    "Let us inspect them for the compiled program.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff944367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor name: cot.predict\n",
      "  Type: Predict\n",
      "  Number of demos: 0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Inspect named predictors and any demonstrations attached to them\n",
    "for name, predictor in compiled_math_program.named_predictors():\n",
    "    demos = getattr(predictor, \"demonstrations\", [])\n",
    "    print(f\"Predictor name: {name}\")\n",
    "    print(f\"  Type: {type(predictor).__name__}\")\n",
    "    print(f\"  Number of demos: {len(demos)}\")\n",
    "    if demos:\n",
    "        print(\"  First demo (truncated):\")\n",
    "        first = demos[0]\n",
    "        print(\"   inputs:\", {k: first.inputs[k] for k in first.inputs})\n",
    "        print(\"   outputs:\", {k: first.outputs[k] for k in first.outputs})\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf748a5",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Look at traces from the compiled program\n",
    "\n",
    "Finally, run the compiled program with tracing enabled to see how the internal calls look now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb5afab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Mike has 7 oranges and buys 3 more. How many does he have?\n",
      "Predicted answer: 10\n",
      "\n",
      "Compiled trace:\n",
      "  Step 0: module=Predict\n",
      "    inputs:  {'question': 'Mike has 7 oranges and buys 3 more. How many does he have?'}\n",
      "    outputs: {'reasoning': 'Mike starts with 7 oranges and buys 3 more. To find the total, add the two quantities: 7 + 3 = 10.', 'answer': '10'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ex = dev_examples[0]\n",
    "\n",
    "with dspy.context(trace=[]):\n",
    "    compiled_pred = compiled_math_program(question=ex.question)\n",
    "    compiled_trace = dspy.settings.trace.copy()\n",
    "\n",
    "print(\"Question:\", ex.question)\n",
    "print(\"Predicted answer:\", getattr(compiled_pred, \"answer\", None))\n",
    "print(\"\\nCompiled trace:\")\n",
    "for j, (mod, inputs, outputs) in enumerate(compiled_trace):\n",
    "    print(f\"  Step {j}: module={type(mod).__name__}\")\n",
    "    print(f\"    inputs:  {inputs}\")\n",
    "    print(f\"    outputs: {dict(outputs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a33657d",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Bonus: a tiny two step program to show multi module traces\n",
    "\n",
    "To show multi step traces, we define a program that:\n",
    "\n",
    "1. rewrites the question into a simpler question\n",
    "2. answers the simpler question\n",
    "\n",
    "This produces traces that have two different modules in them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a03cc04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question: Tom has 3 apples and buys 2 more. How many apples does he have now?\n",
      "Simpler question: Tom has 3 apples and gets 2 more. How many apples does he have now?\n",
      "Answer: 5\n",
      "\n",
      "Two step trace:\n",
      "  Step 0: module=Predict\n",
      "    inputs:  {'question': 'Tom has 3 apples and buys 2 more. How many apples does he have now?'}\n",
      "    outputs: {'reasoning': 'The original question uses the word \"buys,\" which may be slightly more complex for a beginner. Replacing \"buys\" with \"gets\" simplifies the language while keeping the math problem identical. The numbers and structure remain unchanged, making it easier to understand for younger learners.', 'simpler_question': 'Tom has 3 apples and gets 2 more. How many apples does he have now?'}\n",
      "  Step 1: module=Predict\n",
      "    inputs:  {'simpler_question': 'Tom has 3 apples and gets 2 more. How many apples does he have now?'}\n",
      "    outputs: {'reasoning': 'Tom starts with 3 apples and receives 2 more. To find the total, add the two quantities: 3 + 2 = 5.', 'answer': '5'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class RewriteAndSolve(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rewrite = dspy.ChainOfThought(\"question -> simpler_question\")\n",
    "        self.solve = dspy.ChainOfThought(\"simpler_question -> answer\")\n",
    "\n",
    "    def forward(self, question: str):\n",
    "        rewritten = self.rewrite(question=question)\n",
    "        simpler_question = rewritten.simpler_question\n",
    "        solved = self.solve(simpler_question=simpler_question)\n",
    "        return dspy.Prediction(\n",
    "            simpler_question=simpler_question,\n",
    "            answer=solved.answer,\n",
    "        )\n",
    "\n",
    "two_step_program = RewriteAndSolve()\n",
    "\n",
    "example = train_examples[0]\n",
    "with dspy.context(trace=[]):\n",
    "    pred = two_step_program(question=example.question)\n",
    "    trace = dspy.settings.trace.copy()\n",
    "\n",
    "print(\"Original question:\", example.question)\n",
    "print(\"Simpler question:\", getattr(pred, \"simpler_question\", None))\n",
    "print(\"Answer:\", getattr(pred, \"answer\", None))\n",
    "\n",
    "print(\"\\nTwo step trace:\")\n",
    "for j, (mod, inputs, outputs) in enumerate(trace):\n",
    "    print(f\"  Step {j}: module={type(mod).__name__}\")\n",
    "    print(f\"    inputs:  {inputs}\")\n",
    "    print(f\"    outputs: {dict(outputs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9b5826",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Wrap up\n",
    "\n",
    "This notebook showed, on a very small scale:\n",
    "\n",
    "* how to configure DSPy with a local LM\n",
    "* how to run a program and inspect predictions\n",
    "* how to capture traces with `dspy.context(trace=[])`\n",
    "* a simple manual \"backtracking\" style loop for collecting good traces\n",
    "* how `BootstrapFewShotWithRandomSearch` compiles the program and attaches demonstrations\n",
    "* how to inspect the compiled program and its traces\n",
    "\n",
    "You can now swap in your own tasks and metrics, or extend the programs to include retrieval and multi hop reasoning, and reuse the same tracing pattern to show what is happening step by step.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy-testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
